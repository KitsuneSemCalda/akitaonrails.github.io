---
title: AGI ou Skynet não vão chegar tão cedo
date: '2025-06-18T15:00:00-03:00'
slug: agi-ou-skynet-nao-vai-chegar-tao-cedo
tags: 
- agi
- geoffrey hinton
- yann lecun
- sundar pichai
- satya nadella
- openai
draft: false
---

Se não assistiu, recomendo que assista o episódio que participei na última sexta-feira 13/06/2025, no Flow Podcast:

{{< youtube id="sf4Gxf0LiKo" >}}

Eu fiz questão de levantar um ponto que vale a pena complementar agora. Toda vez que você acredita num CEO de empresa que vende I.A., dizendo que "I.A. vai substituir pessoas" ou "AGI está batendo na nossa porta" ou "AGI vai ser catastrófico", você está acreditando num traficante dizendo que cocaína é boa pra sua saúde.

Vamos a novas informações. Sundar Pichai é CEO do Google e um dos que disseram que "30% do código do Google" já é feito com I.A. (sem explicar, claro, "quais 30%" e nem mostrar nada).

Veja o mesmo Sundar Pichai hoje, 18/06/2025 no ["Business Insider"](https://www.businessinsider.com/ai-google-engineers-coding-productive-sundar-pichai-alphabet-2025-6)

![Sundar Pichai](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/sundar-pichai-2025-06-18.jpg)

Interessante como o discurso ficou mais "pé-no-chão" agora. Mas ele não foi o primeiro. Já em fevereiro de 2025 o CEO da Microsoft, Satya Nadella disse o seguinte em [artigos como este](https://futurism.com/microsoft-ceo-ai-generating-no-value):

![Satya Nadella](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/screenshot_18062025_161757.jpg)

E por que esta mudança de discurso, sendo que no fim de 2024 tava todo mundo animado falando só "AGI", "AGI"?

Existe [uma cláusula](https://www.indicpacific.com/post/beyond-agi-promises-decoding-microsoft-openai-s-competition-policy-paradox) na negociação de investimento da Microsoft com a OpenAI (eles investiram USD 14 bilhões) de que a OpenAI pararia de compartilhar tecnologia com a Microsoft, uma vez que "AGI" fosse atingida. Definida como "sistema altamente autônomo que ultrapassa humanos em trabalhos de mais valor econômico". Essa é a definição.

Problema é que isso tira a Microsoft como investidor também. Além disso, recentemente a própria Microsoft começou a se declarar como um concorrente da OpenAI, em vez de parceira, se posicionando junto com um Google ou Claude. Leia os detalhes nesta análise:

[![msft-openai-clause](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/screenshot_18062025_162322.jpg)](https://www.indicpacific.com/post/beyond-agi-promises-decoding-microsoft-openai-s-competition-policy-paradox)
Entenda: toda discussão de "é AGI, não é AGI" tem mais a ver com negociações por baixo dos panos entre as diversas Big Tech envolvidas e seus investidores. Não se relaciona com tecnologia de verdade. Você é só mais macaco de auditório de um show que nem pode assistir. Então está torcendo cegamente, pra variar.

### Geoffrey Hinton e o problema dos especialistas

[![geoffrey hinton interview](https://new-uploads-akitaonrails.s3.us-east-2.amazonaws.com/geoffrey-hinton-interview.jpeg)](https://x.com/10X_AI_/status/1934970334693957852)
Saiu mais uma entrevista do tal Geoffrey Hinton, mais fatalismo, mais "já era" e mais gente gritando pelos cantos achando que Armageddon vai chegar amanhã.

A mídia considera Geoffrey Hinton o "Godfather das I.A."

Por causa disso muitos acreditam em tudo que ele fala - _"Lógico que o cara que inventou I.A. sabe o que vai acontecer"_. Virou um culto.

Eu mesmo falei no Flow que ele inventou Backpropagation - o backbone de treinamento de I.A. -, mas é uma simplificação. O próprio Hinton já esclareceu diversas vezes que ele **não inventou**. O grosso do trabalho foi publicado por **Paul Werbos** e **Shun-Ichi Amari**. Mas de fato, foi o Hinton que popularizou esse trabalho.

Hinton, como professor, teve a sorte de supervisionar alunos que viriam a se tornar brilhantes. Na AlexNet - eu contei no Flow - como foi Ilya Sutskever quem deu a idéia de usar GPUs pra treinar deep neural networks. Ilya, que depois viria a co-fundar a OpenAI - junto com Andrej Karpathy, outro aluno brilhante de HInton e que hoje em dia eu detesto porque ele que cunhou o tal de "Vibe Coding". Karpathy seguindo os passos do seu mentor Hinton e espalhando bullshit.

Ainda na revolução de computer vision, além de usar GPUs, o grande passo de arquitetura foram os _Convolutional Neural Networks_ (CNNs), de **Yann LeCun** - outro aluno de Hinton.

O problema: a mídia dá mais crédito pro supervisor do que pro aluno. E coloca tudo no mesmo saco e fica como se Hinton - sozinho - inventou BackPropagation, CNNs e o AlexNet inteiro. O próprio Prêmio Alan Turing que ele levou, de novo o mérito foi da equipe, mas o nome "Hinton" é mais forte, então foi como se "ele" fosse o maior responsável. Óbvio, desse jeito, qualquer um parece um semi-deus.

Não estou dizendo que ele é burro nem nada disso. Mas não é tudo isso, não é infalível e certamente é um ser humano normal que erra. Não há correlação dos seus trabalhos anteriores com especulações de futuro.

Em outro espectro, existe um prêmio Nobel, Roger Penrose que postula desde os anos 80 ou 90 que **"consciência NÃO é computável"** por exemplo, em seu livro ["The Emperor's New Mind"](https://www.amazon.com.br/Emperors-New-Mind-Concerning-Computers/dp/0192861980). Um Nobel do nível que provou hipóteses de Einstein e trabalhou com Hawkings.

Além disso o próprio Yann LeCun, é do campo que não acredita que as I.A.s atuais vão chegar perto de AGI também. Como eu SEMPRE repito: precisa acontecer alguma nova descoberta - que ainda não sabemos - pra isso acontecer. LeCun tem suas teorias e está na Meta, trabalhando num projeto chamado **V-JEPA** pra ver se ele faz essa descoberta. 

Em vez de ouvir Hinton, ouça um pouco de LeCun:

{{< youtube id="RUnFgu8kH-4" >}} 

Pois bem, se "autoridade" é seu único parâmetro, deveria então acreditar cegamente em Penrose e LeCun também. E agora, como fica?

Este jogo é mais complicado do que sua mente ingênua de seguidor-cego consegue enxergar. Não seja só um macaco de auditório e entenda como as peças de encaixam.

### "Você não consegue provar que AGI não vai existir"

É verdade, ninguém consegue. Ninguém consegue provar que Deus não existe. Ninguém consegue provar que Unicórnios não existem. Isso não significa absolutamente nada, é um erro de lógica.

Pessoas de fora de tecnologia e que nunca estudaram ciência da computação só conseguem ver o que os jornais publicam ou o que influencer fala de bobagem em podcast.

Vamos esclarecer: sim, todos nós - programadores formados em ciência da computação - sabemo que, com a arquitetura atual de LLMs, não há como chegar em AGI. Isso é uma certeza.

O problema é que vocês acham que nem nós sabemos tudo sobre I.A., que existe uma camada misteriosa dentro do modelo que funciona sem que nós saibamos como funciona.

Ou pior, vocês acham que somente alguns poucos nomes considerados importantes como Geoffrey Hinton ou Ilya Sutskever, é que conseguem entender. Entenda uma verdade importante: sim, pra chegar numa Teoria Geral da Relatividade, precisou de gênios gigantes como Albert Einstein, David Hilbert, Kurt Godel, etc. Um deles chegaria nisso, nem todo físico conseguiria.

Mas, uma vez descrita e entendida a Teoria, ela é facilmente ensinada pra alunos de 1o ano de faculdade. Não é nenhum segredo místico que precisa de décadas de treinamento pra entender.

Mesma coisa, perceptrons, redes neurais, backpropagation, gradient descent, RNNs, CNNs, Transformers, etc. Cada uma dessas etapas contou com alguns considerados gênios, como Geoffrey Hinton (que muitos vão dizer que só levou crédito, quem fez o trabalho mesmo foram os Sutkever ou LeCun). Mas uma vez entendido, isso tudo é matéria de graduação de ciência da computação. Você não precisa nem ser um PhD pra entender a maior parte disso.

Os modelos treinados funcionam porque sabemos como ajustar pra funcionar. Parte importante da fase de treinamento é alimentar as redes neurais com milhões de perguntas e respostas PRÉ-PRONTAS. Essa é a fase de INSTRUÇÃO e TODA LLM passa por isso. Ela responde exatamente como cada empresa por trás mandou ela responder, nada a mais que isso.

Ah, mas é tanto parâmetro que não tem com você saber tudo dela. Sim, temos, qualquer bom programador consegue carregar Tensorflow, PyTorch, carregar o modelo e explorar todas as dimensões. Conseguimos partir da resposta e olhar os passos pra trás e ver porque ela se formou daquele jeito. Podemos re-ajustar, podemos mexer quanto quisermos. Eu mostrei parte disso nos posts anteriores.

Todas as etapas do treinamento, da inferência, das otimizações, das metodologias de alinhamento, das estratégias de deployment, tudo está documentado e todos das ciências sabemos exatamente como tudo funciona.

Por isso é assustador que alguém como um Hinton ou Sutskever fiquem fazendo alarmismo sem demonstrar nada. Mostra como eles estão por fora do que está acontecendo. Yann LeCun está correto: está com a mão na massa e ele sabe que a arquitetura atual já bateu no teto e não há muito mais pra seguir daqui. Por isso ele iniciou outros projetos pra tentar conseguir descobrir o que seria uma NOVA arquitetura que leve à próxima etapa. Esse é o projeto V-JEPA 2 da Meta.

Só porque VOCÊ não sabe como as coisas funcionam, não pensem que há margem pra coisas de ficção científica. É a mesma coisa que assistir um filme como "O Chamado" onde espíritos malignos vivem numa fita de VHS e saem da TV, associar que espírito-onda de TV pode ter relação e me dizer "Você não consegue provar que não é possível". É um nível de criancice e falta de conhecimento que não tem como começar a explicar.

Sempre, o **ÔNUS DA PROVA** é de quem VENDE a afirmação. Todo CEO de empresa de IA e todo acadêmico renomado - sem publicar nada - está afirmando isso. Não sou quem tenho que provar: são eles. Ninguém deve tentar provar uma negativa, não faz sentido.

De bônus, voltando pra Yann LeCun. Eu não acompanhava ele, fui ver só hoje o que ele fala. Achei interessante como ele argumenta exatamente as mesmas coisas que eu falo faz mais de 1 ano já. Não quero nem de longe dizer que sou nem uma fração da inteligência dele. Só que partindo dos mesmos princípios, dá pra inevitavelmente chegar nas mesmas conclusões. É inferência pura a partir de primeiros-princípios. Não é uma especulação. Se escrever "2 + 2" tanto eu quanto ele vamos chegar em "4". Todo mundo está dizendo "mas se tiver poder de computação suficiente, um dia vai chegar em 10". Não vai. Assista esta outra entrevista dele, é muito esclarecedor:

{{< youtube id="qvNCVYkHKfg" >}}

O que LeCun e outros - como DeepMind - já disseram, é que LLMs não tem capacidade pra chegar em AGI. Isso é sabido e a DeepMind já provou isso matematicamente. [Este outro video](https://www.youtube.com/watch?v=kpOWmwA6tJc) tem mais detalhes. Mas em resumo, uma LLM é boa como "recuperador de informações". Além do que eu chamo de "completador de texto glorificado", quer dizer que ele só é capaz de completar com informação que ele já viu em treinamento (e combinações aleatórias que às vezes faz parecer que é informação nova, mas não é). A única forma dele conseguir resolver QUALQUER problema é ter um MODELO CAUSAL de TODOS OS PROBLEMAS. Ou seja, ele já saber de ante-mão a resposta pra toda pergunta. Obviamente não temos isso, portanto, é impossível que uma LLM - sendo uma máquina de recuperar informações - nos dê soluções que não existem, como "qual é a nova descoberta que vai permitir AGI?". É um problema não-decidível e não-computável. Matematicamente provado como inviável.

AGI, na arquitetura atual, não vai acontecer. Precisa acontecer alguma nova descoberta. Isso pode ser daqui 2 anos, pode ser daqui 20 anos. Mas não há um caminho possível a partir do que sabemos hoje.
